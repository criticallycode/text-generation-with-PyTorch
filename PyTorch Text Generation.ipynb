{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates an implementation of a text generation method with Pytorch. We'll be using a Recurrent Neural Network or RNN model to train on the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first thing we'll do is import the libraries and functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import time, math\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to preprocess our training text. We'll need to get a list of all the printable characters, as the number of total characters will be used to encode and decode the characters. We then need to use the Unidecode library to represent any special characters as ASCII characters, as this is key to enabling a conversion of text to encoded values. Finally, we need to get the total length of the training text as it will be used to split up our trainign text into random chunks that will be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text_file):\n",
    "    # get all printable characters\n",
    "    print_charas = string.printable\n",
    "    num_charas = len(print_charas)\n",
    "\n",
    "    # print(print_charas)\n",
    "\n",
    "    text_data = unidecode.unidecode(open(text_file, encoding="ISO-8859-1").read())\n",
    "\n",
    "    # figure our the length of the file\n",
    "    # this is important as we will be splitting the text file up into random chunks\n",
    "\n",
    "    len_text = len(text_data)\n",
    "    print(len_text)\n",
    "\n",
    "    return print_charas, num_charas, text_data, len_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the function on our chosen training text to get back our chosen variables. We'll also need a text file to train on. We'll be using half of Frankenstein, available through Project Gutenburg. However, you can use anything you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "printable, num_charas, text_file, text_len = text_preprocess(\"Frankenstein-half.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training process, we're going to be splitting the text data up into chunks. This means we'll have to specify a length/number of characters to set a given chunk to. We'll then have to create a function to actually create the chunks. We'll select a random point starting and ending index in the training text and then get the characters inbetween those two points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the length of the chunk\n",
    "\n",
    "chunk_len = 200\n",
    "\n",
    "def get_rand_chunk():\n",
    "    # select a random starting point from the beginning\n",
    "    # of the file until the end minus chunk len\n",
    "    start_idx = random.randint(0, text_len - chunk_len)\n",
    "    # specify the end point\n",
    "    end_idx = start_idx + chunk_len + 1\n",
    "    return text_file[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be sure the function is working as we intend it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if function works\n",
    "print(get_rand_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now go about creating the model to use for the training and generation. Our model will inherit from the base Module in PyTorch, which allows us to just declare desired arguments/layers like the input size, the hidden size, and the output size. We'll also specify the number of layers we want to use. The first layer in the network will be an encoding layer, which we can set up with `nn.Embedding`. We then need to set up the Gated Recurrent Units, or GRU, that the model will use. Finally, we inherit the Linear and Dropout layers.\n",
    "\n",
    "After declaring all the class variables, we'll create the method for the forward training pass. After declaring the inputs we need to run the inputs through the GRU layer, which returns the output and hidden state. We'll also need a method to initialize the hidden layers on creation, which we'll do by returning a variable full of zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9465\n"
     ]
    }
   ],
   "source": [
    "# inherit from nn.Module\n",
    "class RNN(nn.Module):\n",
    "    # initialize the class with chosen arguments\n",
    "    def __init__(self, in_size, hidden_size, out_size, num_layers=1, drop_prob=0.5):\n",
    "        # make sure to inherit from the base RNN class\n",
    "        super(RNN, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_size = out_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # define the layers\n",
    "        self.encoder = nn.Embedding(in_size, hidden_size)\n",
    "        # hidden size is both input and output here, since we aren't changing the size\n",
    "        # of the input inbetween layers, also takes num layers\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n",
    "        # our purput layer is a linerar layer, will take in hidden size and returns output size\n",
    "        self.decoder = nn.Linear(hidden_size, out_size)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    # now we have to define the forward training pass\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input will be the encodings generated by the encoder, transformed into tensors by view\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        input = self.dropout(input)\n",
    "        # view changes 1 x 1, to length of given sequence\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    # time to define a function to zero states on creation\n",
    "    def init_hidden(self):\n",
    "        # return a variable full of all zeroes\n",
    "        return Variable(torch.zeros(self.num_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already created a function to get training chunks, but we need to convert those to chunks into tensors that our model can train on. We create a tensor with the length of the input string, and then fill in the tensor for every character in the input string after retrieving its index in our list of printable characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_tensor(input):\n",
    "    # initialize the tensor with length of input string\n",
    "    tensor = torch.zeros(len(input)).long()\n",
    "    # for every character in the input string\n",
    "    for i in range(len(input)):\n",
    "        # get the index of the character from the list of printable characters\n",
    "        tensor[i] = printable.index(input[i])\n",
    "    tensor = Variable(tensor)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to evaluate the performance of the network. The network will be fed one character at a time and it will return a probability distribution for the next character in the sequence. This is repeated until the end of the input string. The network requires a hidden state to be able to start generating text, but since the first character doesn't have a hidden state we'll get around this by giving the network a priming string/seed. After the probabilities are returned we'll get the character with the highest probability and return it. This way, we generate one character at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define out priming string, the length of text we want to predict, and the temperature\n",
    "def model_evaluate(prime_str='A', predict_len=100, temp=0.8):\n",
    "\n",
    "    # start off by giving the network a hidden layer with zeroed states\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = chunk_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # need to build up hidden state, start off with the priming string\n",
    "    for c in range(len(prime_str) - 1):\n",
    "        # return the current hidden state\n",
    "        _, hidden = decoder(prime_input[c], hidden)\n",
    "\n",
    "    # make the input whatever the character tensor has generated based on the prime string\n",
    "    inp = prime_input[-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        # use the decoder and get output and hidden values\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        # convert the output data to a tensor with view\n",
    "        # divided by chosen temperature, exp returns exponential\n",
    "        output_dist = output.data.view(-1).div(temp).exp()\n",
    "        # convert into a multinomial\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # now append the predicted character to the string and use that string as the\n",
    "        # next input into the network\n",
    "        predicted_char = printable[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = chunk_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like a function to get the total amount of time that has passed during the trainign, which we can do with the time and math libraries. We'll return both seconds and minutes, and get minutes by dividing by 60 and rounding down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(passed):\n",
    "    sec = time.time() - passed\n",
    "    # round down to nearest minute\n",
    "    minute = math.floor(sec/60)\n",
    "    sec -= minute * 60\n",
    "    return '%dm %ds' % (minute, sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create the training function, which will evidently need the input data and the targets. We start off by using the method to initalize the hidden state that we created earlier, creating a `hidden` variable. We then need to zero the gradients and create a variable to hold the loss. We then get the output and the hidden state for every character in the training chunk. We'll also update teh loss using the criterion, after we unsqueeze it. We can then carry out backpropgation after getting the loss and carry out a step of optimization. Finally, we need to return the loss divided by the length of the training chunk, which will be used in calculating the average loss when we create our final function that carries out training and evaluates the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(input_data, target):\n",
    "\n",
    "    # initialize hidden state\n",
    "    hidden = decoder.init_hidden()\n",
    "    # zero the gradients at the start\n",
    "    decoder.zero_grad()\n",
    "    # set loss to zero\n",
    "    loss = 0\n",
    "\n",
    "    # for every character in the chunk length\n",
    "    # get the output of the model and the hidden state\n",
    "    for char in range(chunk_len):\n",
    "        output, hidden = decoder(input_data[char], hidden)\n",
    "        # update the loss, need to unsqueeze it\n",
    "        loss += criterion(output, target[char].unsqueeze(0))\n",
    "\n",
    "    # do the backprop\n",
    "    loss.backward()\n",
    "    # do a step of optimization\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # return the first element of the loss divided by chunk length\n",
    "    # call the item function to get the item of the data\n",
    "    return loss.data.item()/ chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to declare some important arguments for the model, like the number of training epochs and the number of layers, as well as the training rate. We'll also set a delay for the printing and plotting of statistics. \n",
    "\n",
    "We then need to instantiate our model using our arguments. Finally, we choose an optimization function and a loss criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to declare the training parameters\n",
    "num_epochs = 2000\n",
    "print_delay = 100\n",
    "plot_delay = 100\n",
    "\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "learning_rate = 0.002\n",
    "\n",
    "# what will we use as the decoder, the RNN\n",
    "decoder = RNN(num_charas, hidden_size, num_charas, num_layers, drop_prob=0.2)\n",
    "# choose the optimizer\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# declare the criterion we will use to calculate the loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create a funciton to carry out the training and generation of text. We'll initialize the start time and create variables to hold the total loss and average loss. We then carry out the training for the chosen number of epochs and update the loss, printing statistics every time the chosen number of epoch passes. Finally, we save the model weights for use in the generation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_generate(num_epochs, print_delay, plot_delay):\n",
    "\n",
    "    start = time.time()\n",
    "    total_loss = []\n",
    "    avg_loss = 0\n",
    "\n",
    "    # print the loss and generate text\n",
    "    # for the epochs in the total number of epochs + 1\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        loss = train_loop(*create_training_set())\n",
    "        avg_loss += loss\n",
    "\n",
    "        if epoch % print_delay == 0:\n",
    "            # if epoch num divisible without remainders:\n",
    "            # print the running time divided by number of epochs\n",
    "            print(\"Current running time: \" + get_time(start))\n",
    "            print(\"Epoch:{}, Percent complete: {}%, Loss: {}\".format(epoch, epoch/num_epochs*100, loss))\n",
    "            # this prints the characters that have their probability evaluated\n",
    "            print(model_evaluate('a', 200), '\\n')\n",
    "\n",
    "        if epoch % plot_delay == 0:\n",
    "            total_loss.append(avg_loss / plot_delay)\n",
    "            avg_loss = 0\n",
    "\n",
    "    torch.save(decoder.state_dict(), \"./textgen_model_1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_generate(num_epochs, print_delay, plot_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we have to do is create a function to handle the generation. It's basically the same as the `model_evaluate` function we create earlier, but it takes in our saved weights, meaning we can generate text over and over again without retrainign every time. We just need to declare the model we want to use and load the saved weights in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_gen(prime_str='A', predict_len=100, temp=0.8):\n",
    "    decoder = RNN(num_charas, hidden_size, num_charas, num_layers, drop_prob=0.2)\n",
    "    decoder.load_state_dict(torch.load(\"./textgen_model_1.pth\"))\n",
    "\n",
    "    # start off by giving the network a hidden layer with zeroed states\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = chunk_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # need to build up hidden state, start off with the priming string\n",
    "    for c in range(len(prime_str) - 1):\n",
    "        # return the current hidden state\n",
    "        _, hidden = decoder(prime_input[c], hidden)\n",
    "\n",
    "    # make the input whatever the character tensor has generated based on the prime string\n",
    "    inp = prime_input[-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        # use the decoder and get output and hidden values\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        # convert the output data to a tensor with view\n",
    "        # divided by chosen temperature, exp returns exponential\n",
    "        output_dist = output.data.view(-1).div(temp).exp()\n",
    "        # convert into a multinomial\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # now append the predicted character to the string and use that string as the\n",
    "        # next input into the network\n",
    "        predicted_char = printable[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = chunk_tensor(predicted_char)\n",
    "\n",
    "    print(\"Generated text is:\")\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the generation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
